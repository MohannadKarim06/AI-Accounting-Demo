import streamlit as st
import pandas as pd
import requests
import base64
import json
import os
import time
import hashlib
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas

# =========================
# Page config
# =========================
st.set_page_config(
    page_title="Ù†Ø¸Ø§Ù… Ù…Ø­Ø§Ø³Ø¨ÙŠ Ø°ÙƒÙŠ",
    layout="wide"
)

# =========================
# Arabic font + UI styling
# =========================
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Cairo:wght@300;400;600;800&display=swap');

html, body, [class*="css"]  {
    font-family: 'Cairo', sans-serif;
    direction: rtl;
    text-align: right;
}

h1, h2, h3, h4 {
    font-weight: 700;
}

.stDataFrame {
    direction: rtl;
}
</style>
""", unsafe_allow_html=True)

# =========================
# Constants & paths
# =========================
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
MODEL_VISION = "anthropic/claude-3.5-sonnet"

BASE_CACHE = "/data/cache"
DOC_CACHE = f"{BASE_CACHE}/documents"
QUERY_CACHE = f"{BASE_CACHE}/queries"
REPORT_CACHE = f"{BASE_CACHE}/reports"

for path in [DOC_CACHE, QUERY_CACHE, REPORT_CACHE]:
    os.makedirs(path, exist_ok=True)

ALLOWED_CATEGORIES = [
    "Ù…Ø¨ÙŠØ¹Ø§Øª",
    "Ù…Ø´ØªØ±ÙŠØ§Øª",
    "Ù…ØµØ±ÙˆÙØ§Øª ØªØ´ØºÙŠÙ„",
    "Ø±ÙˆØ§ØªØ¨",
    "Ø¶Ø±Ø§Ø¦Ø¨",
    "Ø®Ø¯Ù…Ø§Øª",
    "Ø£Ø®Ø±Ù‰"
]

# =========================
# Helpers
# =========================
def hash_bytes(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

def encode_file(file):
    return base64.b64encode(file.read()).decode("utf-8")

def call_openrouter(messages, max_tokens=1500):
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": MODEL_VISION,
        "messages": messages,
        "max_tokens": max_tokens
    }
    res = requests.post(OPENROUTER_URL, headers=headers, json=payload, timeout=60)
    res.raise_for_status()
    return res.json()["choices"][0]["message"]["content"]

# =========================
# Document extraction
# =========================
def extract_document(file):
    raw = file.read()
    file_hash = hash_bytes(raw)
    cache_path = f"{DOC_CACHE}/{file_hash}.json"

    if os.path.exists(cache_path):
        time.sleep(2)
        return json.load(open(cache_path, "r", encoding="utf-8"))

    base64_file = base64.b64encode(raw).decode("utf-8")
    mime = file.type

    prompt = f"""
Ø£Ù†Øª Ù…Ø­Ø§Ø³Ø¨ Ù…Ø­ØªØ±Ù ÙÙŠ Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ù…ØµØ±ÙŠ.

Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø¯Ù‚Ø©:
- Ù†ÙˆØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯
- Ø§Ù„ØªØ§Ø±ÙŠØ® (YYYY-MM-DD)
- Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø£Ùˆ Ø§Ù„Ù…ÙˆØ±Ø¯
- Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ (Ø±Ù‚Ù… ÙÙ‚Ø·)
- Ø¶Ø±ÙŠØ¨Ø© Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ© (Ø±Ù‚Ù… ÙÙ‚Ø·)
- Ø§Ù„ØªØµÙ†ÙŠÙ (ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©):
{", ".join(ALLOWED_CATEGORIES)}

Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·.
"""

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{mime};base64,{base64_file}"
                    }
                }
            ]
        }
    ]

    result = call_openrouter(messages)
    data = json.loads(result)
    data["filename"] = file.name

    json.dump(data, open(cache_path, "w", encoding="utf-8"), ensure_ascii=False)
    return data

# =========================
# Semantic query
# =========================
def semantic_query(df, question):
    key = hashlib.sha256((df.to_csv() + question).encode()).hexdigest()
    path = f"{QUERY_CACHE}/{key}.json"

    if os.path.exists(path):
        time.sleep(1.5)
        return json.load(open(path, "r", encoding="utf-8"))

    prompt = f"""
Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
{df.to_string(index=False)}

Ø§Ù„Ø³Ø¤Ø§Ù„:
{question}

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON:
{{
  "answer": "",
  "rows": []
}}
"""

    result = call_openrouter([{"role": "user", "content": prompt}], max_tokens=2000)
    parsed = json.loads(result)

    json.dump(parsed, open(path, "w", encoding="utf-8"), ensure_ascii=False)
    return parsed

# =========================
# Report generation
# =========================
def generate_report(df):
    key = hashlib.sha256(df.to_csv().encode()).hexdigest()
    path = f"{REPORT_CACHE}/{key}.json"

    if os.path.exists(path):
        time.sleep(2)
        return json.load(open(path, "r", encoding="utf-8"))

    prompt = f"""
Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
{df.to_string(index=False)}

Ø£Ù†Ø´Ø¦ ØªÙ‚Ø±ÙŠØ±Ù‹Ø§ Ù…Ø§Ù„ÙŠÙ‹Ø§ Ø§Ø­ØªØ±Ø§ÙÙŠÙ‹Ø§ Ø¨ØµÙŠØºØ© JSON:
{{
 "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†": "",
 "Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„Ø¥ÙŠØ±Ø§Ø¯Ø§Øª": 0,
 "Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„Ù…ØµØ±ÙˆÙØ§Øª": 0,
 "ØµØ§ÙÙŠ_Ø§Ù„Ø±Ø¨Ø­": 0,
 "Ø§Ù„Ù…Ù„Ø®Øµ": ""
}}
"""

    result = call_openrouter([{"role": "user", "content": prompt}], max_tokens=1500)
    parsed = json.loads(result)

    json.dump(parsed, open(path, "w", encoding="utf-8"), ensure_ascii=False)
    return parsed

# =========================
# PDF export
# =========================
def export_pdf(report):
    file_path = "/tmp/report.pdf"
    c = canvas.Canvas(file_path, pagesize=A4)
    text = c.beginText(40, 800)

    for k, v in report.items():
        text.textLine(f"{k}: {v}")

    c.drawText(text)
    c.save()
    return file_path

# =========================
# UI
# =========================
st.title("ğŸ“Š Ù†Ø¸Ø§Ù… Ù…Ø­Ø§Ø³Ø¨ÙŠ Ø°ÙƒÙŠ Ù…ØªÙƒØ§Ù…Ù„")

st.markdown("""
Ù‡Ø°Ø§ Ø¹Ø±Ø¶ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù†Ø¸Ø§Ù… **ÙŠØ¯Ø®Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø§Ø³Ø¨ÙŠØ©**  
ÙˆÙ„ÙŠØ³ Ù…Ø¬Ø±Ø¯ Ø£Ø¯Ø§Ø© Ù…Ù†ÙØµÙ„Ø©.
""")

st.divider()

# -------- Step 1
st.header("1ï¸âƒ£ Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø§Ø³Ø¨ÙŠØ©")

files = st.file_uploader(
    "Ø§Ø±ÙØ¹ ÙÙˆØ§ØªÙŠØ±ØŒ Ø¥ÙŠØµØ§Ù„Ø§ØªØŒ Ù…ØµØ±ÙˆÙØ§Øª (ØµÙˆØ± Ø£Ùˆ PDF)",
    accept_multiple_files=True
)

records = []
if files:
    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª..."):
        for f in files:
            records.append(extract_document(f))

if records:
    df = pd.DataFrame(records)
    st.success("ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
    st.dataframe(df)

    # -------- Step 2
    st.header("2ï¸âƒ£ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ")
    question = st.text_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ø«Ø§Ù„: Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª ÙÙŠ ÙŠÙˆÙ… Ù…Ø¹ÙŠÙ†)")

    if question:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."):
            answer = semantic_query(df, question)
        st.markdown(f"**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:** {answer['answer']}")
        if answer["rows"]:
            st.dataframe(pd.DataFrame(answer["rows"]))

    # -------- Step 3
    st.header("3ï¸âƒ£ ØªÙ‚Ø±ÙŠØ± Ù…Ø§Ù„ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠ")

    if st.button("Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ±"):
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±..."):
            report = generate_report(df)
        st.json(report)

        pdf_path = export_pdf(report)
        with open(pdf_path, "rb") as f:
            st.download_button(
                "ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± PDF",
                f,
                file_name="financial_report.pdf"
            )

st.divider()
st.caption("Ù†Ù…ÙˆØ°Ø¬ ØªØ¬Ø±ÙŠØ¨ÙŠ â€” ÙŠÙˆØ¶Ø­ Ø§Ù„Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
